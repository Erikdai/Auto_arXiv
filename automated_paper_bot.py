#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–Agentè®ºæ–‡æœºå™¨äºº - æœ¬åœ°å®šæ—¶è¿è¡Œç‰ˆæœ¬

åŠŸèƒ½ï¼š
1. æ¯å¤©ä¸‹åˆ5ç‚¹è‡ªåŠ¨è¿è¡Œ
2. çˆ¬å–è¿‡å»24å°æ—¶å†…å‘å¸ƒçš„Agentç›¸å…³è®ºæ–‡
3. è‡ªåŠ¨å‘å¸ƒåˆ°Twitter
4. å®Œå–„çš„é”™è¯¯å¤„ç†å’ŒæœåŠ¡æ£€æŸ¥
"""
import psycopg2
import tweepy
import subprocess
import os

import time
import requests
import logging
from datetime import datetime, timedelta
from config import DB_CONFIG, TWITTER_API_CONFIG, GROQ_CONFIG
from paper_analyzer import PaperAnalyzer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('paper_bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AutomatedPaperBot:
    def __init__(self):
        """åˆå§‹åŒ–æœºå™¨äºº"""
        self.twitter_client = None
        self.connection = None
        self.cursor = None
        self.analyzer = None
        
        logger.info("ğŸ¤– è‡ªåŠ¨åŒ–Agentè®ºæ–‡æœºå™¨äººåˆå§‹åŒ–")

    def health_check(self):
        """æ‰§è¡ŒæœåŠ¡å¯ç”¨æ€§æ£€æŸ¥"""
        logger.info("ğŸ” å¼€å§‹æœåŠ¡å¥åº·æ£€æŸ¥...")
        
        # æ£€æŸ¥arXivç½‘ç«™å¯è®¿é—®æ€§
        if not self._check_arxiv_availability():
            logger.error("âŒ arXivç½‘ç«™ä¸å¯è®¿é—®ï¼Œåœæ­¢æ‰§è¡Œ")
            return False
        
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        if not self._check_database_connection():
            logger.error("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
            return False
        
        # æ£€æŸ¥Twitter API
        if not self._check_twitter_api():
            logger.error("âŒ Twitter APIè®¤è¯å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
            return False
        
        logger.info("âœ… æ‰€æœ‰æœåŠ¡æ£€æŸ¥é€šè¿‡")
        return True

    def _check_arxiv_availability(self):
        """æ£€æŸ¥arXivç½‘ç«™å¯è®¿é—®æ€§"""
        try:
            logger.info("  æ£€æŸ¥arXivç½‘ç«™...")
            response = requests.get("https://arxiv.org", timeout=10)
            if response.status_code == 200:
                logger.info("  âœ… arXivç½‘ç«™æ­£å¸¸")
                return True
            else:
                logger.error(f"  âŒ arXivè¿”å›çŠ¶æ€ç : {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"  âŒ arXivç½‘ç«™è®¿é—®å¤±è´¥: {e}")
            return False

    def _check_database_connection(self):
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
        try:
            logger.info("  æ£€æŸ¥æ•°æ®åº“è¿æ¥...")
            self.connection = psycopg2.connect(**DB_CONFIG)
            self.cursor = self.connection.cursor()
            
            # æµ‹è¯•æŸ¥è¯¢
            self.cursor.execute("SELECT 1;")
            result = self.cursor.fetchone()
            
            if result:
                logger.info("  âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
                return True
            else:
                logger.error("  âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥")
                return False
        except Exception as e:
            logger.error(f"  âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False

    def _check_twitter_api(self):
        """æ£€æŸ¥Twitter APIè®¤è¯"""
        try:
            logger.info("  æ£€æŸ¥Twitter API...")
            self.twitter_client = tweepy.Client(
                consumer_key=TWITTER_API_CONFIG['consumer_key'],
                consumer_secret=TWITTER_API_CONFIG['consumer_secret'],
                access_token=TWITTER_API_CONFIG['access_token'],
                access_token_secret=TWITTER_API_CONFIG['access_token_secret'],
                wait_on_rate_limit=True
            )
            
            # æµ‹è¯•APIè°ƒç”¨
            me = self.twitter_client.get_me()
            if me.data:
                logger.info(f"  âœ… Twitter APIæ­£å¸¸ (@{me.data.username})")
                return True
            else:
                logger.error("  âŒ Twitter APIè®¤è¯å¤±è´¥")
                return False
        except Exception as e:
            logger.error(f"  âŒ Twitter APIæ£€æŸ¥å¤±è´¥: {e}")
            return False

    def crawl_last_24h_papers(self):
        """çˆ¬å–è¿‡å»24å°æ—¶å†…å‘å¸ƒçš„Agentç›¸å…³è®ºæ–‡"""
        try:
            logger.info("ğŸ•·ï¸ å¼€å§‹çˆ¬å–è¿‡å»24å°æ—¶çš„Agentè®ºæ–‡...")
            
            # ä¿®æ”¹çˆ¬è™«é…ç½®ï¼Œçˆ¬å–è¿‡å»24å°æ—¶çš„è®ºæ–‡
            process = subprocess.Popen(
                ["scrapy", "crawl", "arxiv", "-s", "CLOSESPIDER_TIMEOUT=0"],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                cwd=os.getcwd()
            )
            
            # å®æ—¶è¯»å–è¾“å‡º
            output_lines = []
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    line = output.strip()
                    output_lines.append(line)
                    # æ˜¾ç¤ºé‡è¦ä¿¡æ¯
                    if any(keyword in line for keyword in ['Agentè®ºæ–‡', 'çˆ¬å–', 'è®ºæ–‡å·²ä¿å­˜', 'Spider closed']):
                        logger.info(f"   {line}")
            
            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            return_code = process.poll()
            
            if return_code == 0:
                logger.info("âœ… çˆ¬è™«è¿è¡ŒæˆåŠŸ")
                
                # æ£€æŸ¥çˆ¬å–ç»“æœ - è·å–è¿‡å»24å°æ—¶çš„è®ºæ–‡
                yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
                today = datetime.now().strftime('%Y-%m-%d')
                
                self.cursor.execute("""
                    SELECT COUNT(*) FROM papers 
                    WHERE added_at >= %s AND added_at <= %s;
                """, (yesterday, today))
                
                new_count = self.cursor.fetchone()[0]
                logger.info(f"ğŸ“Š çˆ¬å–åˆ° {new_count} ç¯‡Agentç›¸å…³è®ºæ–‡")
                
                return new_count > 0
            else:
                logger.error(f"âŒ çˆ¬è™«è¿è¡Œå¤±è´¥ï¼Œè¿”å›ç : {return_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–è®ºæ–‡å¤±è´¥: {e}")
            return False

    def get_last_24h_papers(self):
        """è·å–è¿‡å»24å°æ—¶çš„Agentè®ºæ–‡"""
        try:
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            today = datetime.now().strftime('%Y-%m-%d')
            
            self.cursor.execute("""
                SELECT id, category, title, authors, abstract, url, added_at
                FROM papers 
                WHERE added_at >= %s AND added_at <= %s
                AND (category = 'cs.CL' OR category = 'cs.AI')
                ORDER BY sn DESC;
            """, (yesterday, today))
            
            papers = self.cursor.fetchall()
            logger.info(f"ğŸ“š è¿‡å»24å°æ—¶æœ‰ {len(papers)} ç¯‡Agentè®ºæ–‡")
            return papers
        except Exception as e:
            logger.error(f"âŒ è·å–è®ºæ–‡å¤±è´¥: {e}")
            return []

    def analyze_and_select_papers(self, papers, max_papers=10):
        """ä½¿ç”¨LLMåˆ†æå¹¶é€‰å‡ºæœ€ç›¸å…³çš„è®ºæ–‡"""
        logger.info(f"ğŸ¤– ä½¿ç”¨LLMåˆ†æAgentè®ºæ–‡ç›¸å…³æ€§...")
        
        # åˆå§‹åŒ–AIåˆ†æå™¨
        try:
            self.analyzer = PaperAnalyzer()
            logger.info("âœ… AIåˆ†æå™¨å·²å¯ç”¨")
        except Exception as e:
            logger.error(f"âŒ AIåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return []
        
        scored_papers = []
        
        for i, paper in enumerate(papers, 1):
            paper_id, category, title, authors, abstract, url, added_at = paper
            
            logger.info(f"  åˆ†æ {i}/{len(papers)}: {title[:50]}...")
            
            if not abstract or len(abstract.strip()) < 50:
                logger.info("    âš ï¸ æ‘˜è¦å¤ªçŸ­ï¼Œè·³è¿‡")
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºè§†è§‰ç›¸å…³è®ºæ–‡ï¼ˆæ’é™¤ï¼‰
            vision_keywords = [
                'vision', 'visual', 'image', 'video', 'computer vision', 'cv', 
                'object detection', 'segmentation', 'recognition', 'vqa', 
                'multimodal', 'image generation', 'visual question answering'
            ]
            title_lower = title.lower()
            abstract_lower = abstract.lower()
            
            is_vision_related = any(keyword in title_lower or keyword in abstract_lower for keyword in vision_keywords)
            
            if is_vision_related:
                logger.info("    ğŸš« è§†è§‰ç›¸å…³è®ºæ–‡ï¼Œè·³è¿‡")
                continue
            
            try:
                # ä½¿ç”¨AIåˆ†æè®ºæ–‡ç›¸å…³æ€§
                analysis = self.analyzer.analyze_abstract(abstract, "Agent, Multi-Agent Systems, Agentic AI, LLM Agents")
                
                # åªä¿ç•™é«˜åˆ†è®ºæ–‡ï¼ˆ8åˆ†ä»¥ä¸Šï¼‰- æ›´ä¸¥æ ¼çš„Agentè®ºæ–‡ç­›é€‰
                if analysis['relevance_score'] < 8:
                    logger.info(f"    ğŸ“Š è¯„åˆ†è¿‡ä½: {analysis['relevance_score']}/10ï¼Œè·³è¿‡ï¼ˆéœ€è¦â‰¥8åˆ†ï¼‰")
                    continue
                
                scored_papers.append({
                    'paper': paper,
                    'score': analysis['relevance_score'],
                    'analysis': analysis
                })
                logger.info(f"    âœ… è¯„åˆ†: {analysis['relevance_score']}/10")
                
            except Exception as e:
                logger.error(f"    âŒ åˆ†æå¤±è´¥: {e}")
                continue
        
        # æŒ‰è¯„åˆ†æ’åºï¼Œå–å‰Nç¯‡
        scored_papers.sort(key=lambda x: x['score'], reverse=True)
        top_papers = scored_papers[:max_papers]
        
        logger.info(f"âœ… é€‰å‡ºå‰ {len(top_papers)} ç¯‡æœ€ç›¸å…³çš„Agentè®ºæ–‡")
        for i, paper_data in enumerate(top_papers, 1):
            paper = paper_data['paper']
            score = paper_data['score']
            logger.info(f"  {i}. [{score}/10] {paper[2][:60]}...")
        
        return top_papers

    def post_papers_to_twitter(self, papers):
        """å‘å¸ƒè®ºæ–‡åˆ°Twitter - å•æ¡æ¨æ–‡æ ¼å¼ï¼Œä¼˜å…ˆä¿è¯æè¿°å’Œé“¾æ¥å®Œæ•´"""
        if not papers:
            logger.info("ğŸ“ æ²¡æœ‰è®ºæ–‡å¯å‘å¸ƒ")
            return
        
        logger.info(f"ğŸ“± å‡†å¤‡å‘å¸ƒ {len(papers)} ç¯‡è®ºæ–‡æ¨æ–‡")
        
        try:
            successful_tweets = 0
            tweet_ids = []
            
            for i, paper_data in enumerate(papers, 1):
                paper = paper_data['paper']
                paper_id, category, title, authors, abstract, url, added_at = paper
                
                logger.info(f"\nğŸ“± å‘å¸ƒç¬¬ {i} ç¯‡è®ºæ–‡æ¨æ–‡")
                logger.info(f"ğŸ“„ {title[:50]}...")
                
                # ç”Ÿæˆè®ºæ–‡æè¿°
                try:
                    description = self.analyzer.generate_description(title, abstract)
                    logger.info(f"    ğŸ“ ç”Ÿæˆæè¿° ({len(description)} å­—ç¬¦): {description}")
                except Exception as e:
                    logger.error(f"    âŒ æè¿°ç”Ÿæˆå¤±è´¥: {e}")
                    description = "Novel AI agent approach solving key challenges."
                
                # æ¸…ç†æ ‡é¢˜ï¼Œå»æ‰"Title:"å‰ç¼€èŠ‚çœå­—ç¬¦
                clean_title = title.replace("Title:", "").strip()
                
                # æ™ºèƒ½å­—ç¬¦åˆ†é…ï¼šä¼˜å…ˆä¿è¯æè¿°å’Œé“¾æ¥å®Œæ•´ï¼ŒåŠ¨æ€åˆ†é…æ ‡é¢˜ç©ºé—´
                # è®¡ç®—å›ºå®šéƒ¨åˆ†ï¼šæè¿° + é“¾æ¥ + æ¢è¡Œç¬¦ï¼ˆè¿™äº›éƒ¨åˆ†ä¸èƒ½è¢«æˆªæ–­ï¼‰
                fixed_chars = len(description) + len(url) + 4  # 4ä¸ªæ¢è¡Œç¬¦
                
                # è®¡ç®—æ ‡é¢˜å¯ç”¨çš„å­—ç¬¦æ•°
                available_for_title = 280 - fixed_chars
                
                # å¦‚æœæ ‡é¢˜éœ€è¦æˆªæ–­
                if len(clean_title) > available_for_title:
                    if available_for_title >= 10:  # ç¡®ä¿æ ‡é¢˜è‡³å°‘æœ‰10ä¸ªå­—ç¬¦æ‰æ˜¾ç¤º
                        truncated_title = clean_title[:available_for_title-3] + "..."
                        tweet_content = f"""{truncated_title}

{description}

{url}"""
                        logger.info(f"âš ï¸ æ ‡é¢˜æˆªæ–­è‡³ {available_for_title-3} å­—ç¬¦ä»¥é€‚åº” {len(description)} å­—ç¬¦çš„æè¿°")
                        logger.info(f"âœ… æ™ºèƒ½åˆ†é…ï¼šæè¿° {len(description)} å­—ç¬¦ï¼Œæ ‡é¢˜ {len(truncated_title)} å­—ç¬¦")
                    else:
                        # æç«¯æƒ…å†µï¼šæè¿°å¤ªé•¿ï¼Œä¸æ˜¾ç¤ºæ ‡é¢˜ï¼Œåªæ˜¾ç¤ºæè¿°å’Œé“¾æ¥
                        tweet_content = f"""{description}

{url}"""
                        logger.warning(f"âš ï¸ æè¿°å¾ˆé•¿ ({len(description)} å­—ç¬¦)ï¼Œå¯ç”¨ç©ºé—´ä¸è¶³ ({available_for_title} å­—ç¬¦)ï¼Œä¸æ˜¾ç¤ºæ ‡é¢˜")
                        logger.info(f"âœ… æç®€æ ¼å¼ï¼šä»…æ˜¾ç¤ºæè¿°å’Œé“¾æ¥")
                else:
                    # æ ‡é¢˜æ— éœ€æˆªæ–­ï¼Œæ˜¾ç¤ºå®Œæ•´æ ‡é¢˜
                    tweet_content = f"""{clean_title}

{description}

{url}"""
                    remaining_chars = available_for_title - len(clean_title)
                    logger.info(f"âœ… æ ‡é¢˜å®Œæ•´æ˜¾ç¤ºï¼Œå‰©ä½™ {remaining_chars} å­—ç¬¦ç©ºé—´")
                
                final_length = len(tweet_content)
                if final_length > 280:
                    logger.error(f"âŒ æ¨æ–‡ä»è¶…é™ {final_length}/280 å­—ç¬¦ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
                else:
                    logger.info(f"âœ… æ¨æ–‡é•¿åº¦ç¬¦åˆè¦æ±‚ {final_length}/280 å­—ç¬¦")
                
                logger.info(f"ğŸ“ æ¨æ–‡å†…å®¹ ({len(tweet_content)} å­—ç¬¦):")
                logger.info("=" * 40)
                logger.info(tweet_content)
                logger.info("=" * 40)
                
                try:
                    # å‘å¸ƒæ¨æ–‡ï¼Œæœ€å¤šé‡è¯•3æ¬¡
                    for attempt in range(3):
                        try:
                            response = self.twitter_client.create_tweet(text=tweet_content)
                            if response.data:
                                tweet_id = response.data['id']
                                tweet_ids.append(tweet_id)
                                successful_tweets += 1
                                logger.info(f"âœ… æ¨æ–‡ {i} å‘å¸ƒæˆåŠŸï¼ID: {tweet_id}")
                                break
                            else:
                                logger.error(f"âŒ æ¨æ–‡ {i} å‘å¸ƒå¤±è´¥ - æ— å“åº”æ•°æ®")
                        except Exception as tweet_error:
                            logger.error(f"âŒ æ¨æ–‡ {i} å‘å¸ƒå¤±è´¥ (å°è¯• {attempt + 1}/3): {tweet_error}")
                            if attempt < 2:
                                time.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
                            else:
                                break
                    
                    # æ¨æ–‡é—´éš”10ç§’
                    if i < len(papers):
                        logger.info("â³ ç­‰å¾…10ç§’...")
                        time.sleep(10)
                        
                except Exception as tweet_error:
                    logger.error(f"âŒ æ¨æ–‡ {i} å‘å¸ƒå¤±è´¥: {tweet_error}")
                    continue
            
            logger.info(f"\nğŸ‰ æ¨æ–‡å‘å¸ƒå®Œæˆï¼")
            logger.info(f"ğŸ“Š æˆåŠŸå‘å¸ƒ {successful_tweets}/{len(papers)} æ¡æ¨æ–‡")
            if tweet_ids:
                logger.info(f"ğŸ”— æ¨æ–‡IDs: {', '.join(tweet_ids)}")
                
        except Exception as e:
            logger.error(f"âŒ æ¨æ–‡å‘å¸ƒå¤±è´¥: {e}")

    def daily_task(self):
        """æ¯æ—¥è‡ªåŠ¨ä»»åŠ¡ï¼šçˆ¬å–è¿‡å»24å°æ—¶çš„è®ºæ–‡å¹¶å‘å¸ƒ"""
        logger.info("ğŸŒ… å¼€å§‹æ¯æ—¥è‡ªåŠ¨ä»»åŠ¡")
        logger.info("=" * 60)
        
        try:
            # æ­¥éª¤1: æœåŠ¡å¥åº·æ£€æŸ¥
            if not self.health_check():
                logger.error("âŒ æœåŠ¡æ£€æŸ¥å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                return
            
            # æ­¥éª¤2: çˆ¬å–è¿‡å»24å°æ—¶çš„Agentè®ºæ–‡
            if not self.crawl_last_24h_papers():
                logger.error("âŒ çˆ¬å–å¤±è´¥ï¼Œä»Šæ—¥æ— æ¨æ–‡")
                return
            
            # æ­¥éª¤3: è·å–è®ºæ–‡
            papers = self.get_last_24h_papers()
            if not papers:
                logger.info("ğŸ“ æ²¡æœ‰æ–°çš„Agentè®ºæ–‡ï¼Œä»Šæ—¥æ— æ¨æ–‡")
                return
            
            # æ­¥éª¤4: AIåˆ†æå¹¶é€‰å‡ºæœ€ç›¸å…³çš„è®ºæ–‡
            top_papers = self.analyze_and_select_papers(papers, max_papers=10)
            if not top_papers:
                logger.info("ğŸ“ æ²¡æœ‰æ‰¾åˆ°é«˜è´¨é‡Agentè®ºæ–‡ï¼Œä»Šæ—¥æ— æ¨æ–‡")
                return
            
            # æ­¥éª¤5: å‘å¸ƒåˆ°Twitter
            self.post_papers_to_twitter(top_papers)
            
            logger.info("âœ… æ¯æ—¥ä»»åŠ¡å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¯æ—¥ä»»åŠ¡å¤±è´¥: {e}")
            self._create_error_report(e)
        finally:
            self.close()

    def _create_error_report(self, error):
        """åˆ›å»ºé”™è¯¯æŠ¥å‘Šæ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            error_file = f"error_report_{timestamp}.txt"
            
            with open(error_file, 'w', encoding='utf-8') as f:
                f.write(f"é”™è¯¯æŠ¥å‘Š - {datetime.now()}\n")
                f.write("=" * 50 + "\n")
                f.write(f"é”™è¯¯ä¿¡æ¯: {str(error)}\n")
                f.write(f"é”™è¯¯ç±»å‹: {type(error).__name__}\n")
                f.write("=" * 50 + "\n")
            
            logger.info(f"ğŸ“„ é”™è¯¯æŠ¥å‘Šå·²ä¿å­˜: {error_file}")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºé”™è¯¯æŠ¥å‘Šå¤±è´¥: {e}")



    def run_test_mode(self):
        """æµ‹è¯•æ¨¡å¼ - ç«‹å³æ‰§è¡Œä¸€æ¬¡å®Œæ•´æµç¨‹"""
        logger.info("ğŸ§ª æµ‹è¯•æ¨¡å¼ - ç«‹å³æ‰§è¡Œ")
        logger.info("=" * 60)
        
        # æ‰§è¡Œæ¯æ—¥ä»»åŠ¡
        self.daily_task()

    def run_analyze_mode(self):
        """åˆ†ææ¨¡å¼ - åªåˆ†æè®ºæ–‡ï¼Œä¸å‘æ¨æ–‡"""
        logger.info("ğŸ” åˆ†ææ¨¡å¼ - åªåˆ†æè®ºæ–‡ï¼Œä¸å‘æ¨æ–‡")
        logger.info("=" * 60)
        
        try:
            # æ­¥éª¤1: æœåŠ¡å¥åº·æ£€æŸ¥
            if not self.health_check():
                logger.error("âŒ æœåŠ¡æ£€æŸ¥å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                return
            
            # æ­¥éª¤2: çˆ¬å–è¿‡å»24å°æ—¶çš„Agentè®ºæ–‡
            if not self.crawl_last_24h_papers():
                logger.error("âŒ çˆ¬å–å¤±è´¥ï¼Œæ— æ³•åˆ†æ")
                return
            
            # æ­¥éª¤3: è·å–è®ºæ–‡
            papers = self.get_last_24h_papers()
            if not papers:
                logger.info("ğŸ“ æ²¡æœ‰æ–°çš„Agentè®ºæ–‡å¯åˆ†æ")
                return
            
            # æ­¥éª¤4: AIåˆ†æå¹¶é€‰å‡ºæœ€ç›¸å…³çš„è®ºæ–‡
            top_papers = self.analyze_and_select_papers(papers, max_papers=10)
            if not top_papers:
                logger.info("ğŸ“ æ²¡æœ‰æ‰¾åˆ°é«˜è´¨é‡Agentè®ºæ–‡")
                return
            
            # æ­¥éª¤5: ç”Ÿæˆæ¨æ–‡å†…å®¹é¢„è§ˆï¼ˆä¸å®é™…å‘å¸ƒï¼‰
            logger.info("ğŸ¯ åˆ†æå®Œæˆï¼ç”Ÿæˆæ¨æ–‡å†…å®¹é¢„è§ˆï¼š")
            logger.info("=" * 60)
            
            for i, paper_data in enumerate(top_papers, 1):
                paper = paper_data['paper']
                score = paper_data['score']
                analysis = paper_data['analysis']
                
                paper_id, category, title, authors, abstract, url, added_at = paper
                
                logger.info(f"\nğŸ“„ è®ºæ–‡ {i}: è¯„åˆ† {score}/10")
                logger.info(f"æ ‡é¢˜: {title}")
                logger.info(f"ä½œè€…: {authors[:80]}...")
                logger.info(f"åˆ†ç±»: {category}")
                logger.info(f"é“¾æ¥: {url}")
                logger.info(f"AIåˆ†æ: {analysis.get('analysis', 'N/A')[:100]}...")
                
                # ç”Ÿæˆæ¨æ–‡å†…å®¹
                try:
                    description = self.analyzer.generate_description(title, abstract)
                    logger.info(f"ğŸ“ ç”Ÿæˆæè¿° ({len(description)} å­—ç¬¦): {description}")
                except Exception as e:
                    logger.error(f"âŒ æè¿°ç”Ÿæˆå¤±è´¥: {e}")
                    description = "Novel AI agent approach solving key challenges."
                
                # æ¸…ç†æ ‡é¢˜ï¼Œå»æ‰"Title:"å‰ç¼€èŠ‚çœå­—ç¬¦
                clean_title = title.replace("Title:", "").strip()
                
                # æ„å»ºæ¨æ–‡å†…å®¹
                tweet_content = f"""{clean_title}

{description}

{url}"""
                
                # æ™ºèƒ½å­—ç¬¦åˆ†é…ï¼šä¼˜å…ˆä¿è¯æè¿°å’Œé“¾æ¥å®Œæ•´ï¼ŒåŠ¨æ€åˆ†é…æ ‡é¢˜ç©ºé—´
                # è®¡ç®—å›ºå®šéƒ¨åˆ†ï¼šæè¿° + é“¾æ¥ + æ¢è¡Œç¬¦ï¼ˆè¿™äº›éƒ¨åˆ†ä¸èƒ½è¢«æˆªæ–­ï¼‰
                fixed_chars = len(description) + len(url) + 4  # 4ä¸ªæ¢è¡Œç¬¦
                
                # è®¡ç®—æ ‡é¢˜å¯ç”¨çš„å­—ç¬¦æ•°
                available_for_title = 280 - fixed_chars
                
                # å¦‚æœæ ‡é¢˜éœ€è¦æˆªæ–­
                if len(clean_title) > available_for_title:
                    if available_for_title >= 10:  # ç¡®ä¿æ ‡é¢˜è‡³å°‘æœ‰10ä¸ªå­—ç¬¦æ‰æ˜¾ç¤º
                        truncated_title = clean_title[:available_for_title-3] + "..."
                        tweet_content = f"""{truncated_title}

{description}

{url}"""
                        logger.info(f"âš ï¸ æ ‡é¢˜æˆªæ–­è‡³ {available_for_title-3} å­—ç¬¦ä»¥é€‚åº” {len(description)} å­—ç¬¦çš„æè¿°")
                        logger.info(f"âœ… æ™ºèƒ½åˆ†é…ï¼šæè¿° {len(description)} å­—ç¬¦ï¼Œæ ‡é¢˜ {len(truncated_title)} å­—ç¬¦")
                    else:
                        # æç«¯æƒ…å†µï¼šæè¿°å¤ªé•¿ï¼Œä¸æ˜¾ç¤ºæ ‡é¢˜ï¼Œåªæ˜¾ç¤ºæè¿°å’Œé“¾æ¥
                        tweet_content = f"""{description}

{url}"""
                        logger.warning(f"âš ï¸ æè¿°å¾ˆé•¿ ({len(description)} å­—ç¬¦)ï¼Œå¯ç”¨ç©ºé—´ä¸è¶³ ({available_for_title} å­—ç¬¦)ï¼Œä¸æ˜¾ç¤ºæ ‡é¢˜")
                        logger.info(f"âœ… æç®€æ ¼å¼ï¼šä»…æ˜¾ç¤ºæè¿°å’Œé“¾æ¥")
                else:
                    # æ ‡é¢˜æ— éœ€æˆªæ–­ï¼Œæ˜¾ç¤ºå®Œæ•´æ ‡é¢˜
                    tweet_content = f"""{clean_title}

{description}

{url}"""
                    remaining_chars = available_for_title - len(clean_title)
                    logger.info(f"âœ… æ ‡é¢˜å®Œæ•´æ˜¾ç¤ºï¼Œå‰©ä½™ {remaining_chars} å­—ç¬¦ç©ºé—´")
                
                final_length = len(tweet_content)
                if final_length > 280:
                    logger.error(f"âŒ æ¨æ–‡ä»è¶…é™ {final_length}/280 å­—ç¬¦ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
                else:
                    logger.info(f"âœ… æ¨æ–‡é•¿åº¦ç¬¦åˆè¦æ±‚ {final_length}/280 å­—ç¬¦")
                
                # æ˜¾ç¤ºæœ€ç»ˆæ¨æ–‡å†…å®¹
                logger.info(f"\nğŸ“± æ¨æ–‡å†…å®¹é¢„è§ˆ ({len(tweet_content)}/280 å­—ç¬¦):")
                logger.info("=" * 40)
                logger.info(tweet_content)
                logger.info("=" * 40)
                logger.info("-" * 60)
            
            logger.info(f"\nâœ… åˆ†æå®Œæˆï¼å…±ç­›é€‰å‡º {len(top_papers)} ç¯‡é«˜è´¨é‡Agentè®ºæ–‡ï¼Œå·²ç”Ÿæˆæ¨æ–‡å†…å®¹é¢„è§ˆ")
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æä»»åŠ¡å¤±è´¥: {e}")
            self._create_error_report(e)
        finally:
            self.close()

    def close(self):
        """å…³é—­è¿æ¥"""
        try:
            if self.analyzer:
                self.analyzer.close()
            if self.cursor:
                self.cursor.close()
            if self.connection:
                self.connection.close()
            logger.info("ğŸ”’ è¿æ¥å·²å…³é—­")
        except Exception as e:
            logger.error(f"âŒ å…³é—­è¿æ¥å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    try:
        bot = AutomatedPaperBot()
        
        if len(sys.argv) > 1:
            if sys.argv[1] == "test":
                # æµ‹è¯•æ¨¡å¼ - å®Œæ•´æµç¨‹åŒ…æ‹¬å‘æ¨
                bot.run_test_mode()
            elif sys.argv[1] == "analyze":
                # åˆ†ææ¨¡å¼ - åªåˆ†æä¸å‘æ¨
                bot.run_analyze_mode()
        else:
            # é»˜è®¤ï¼šç›´æ¥æ‰§è¡Œä»»åŠ¡ï¼ˆç”¨äºWindowsä»»åŠ¡è®¡åˆ’ç¨‹åºï¼‰
            bot.daily_task()
            
    except Exception as e:
        logger.error(f"âŒ æœºå™¨äººå¯åŠ¨å¤±è´¥: {e}")
        logger.error("ğŸ’¡ è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶å’Œç½‘ç»œè¿æ¥")

if __name__ == "__main__":
    main()